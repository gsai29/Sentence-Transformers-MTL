# Sentence Transformers & Multi-Task Learning Project

## Project Overview
This repository contains implementations for a series of tasks aimed at exploring advanced functionalities of sentence transformers and multi-task learning models using neural networks. The tasks demonstrate model architectures, layer-wise learning rate configurations, and transfer learning considerations.

## Contents
- **Task 1** - Sentence Transformer Implementation: Features a model that encodes sentences into fixed-length embeddings. Detailed instructions and a Docker setup are provided to replicate results.
- **Task 2** - Multi-Task Learning Expansion: Contains a multi-task learning model that handles sentence classification and sentiment analysis.
- **Task 3** - Training Considerations: Discusses different strategies for freezing layers during training and their implications.
- **Task 4** - Layer-wise Learning Rate Implementation: Demonstrates the configuration of layer-wise learning rates within a neural network model.

## Getting Started
To begin exploring the projects:
1. Clone this repository:
```bash
git clone https://github.com/gsai29/sentence-transformers-mtl.git
```
2. Navigate into each task directory to find specific instructions on running the models and exploring the configurations.
